"""
ArXiv Quantum Physics Paper Triage & Summarization Agent
Visual Studio Code Version

by John Akwei, Senior Data Scientist
ContextBase: https://contextbase.github.io

This agent solves the problem of information overload in quantum physics research 
by automatically retrieving, analyzing, and summarizing recent papers from ArXiv.

Architecture: Multi-agent sequential pipeline with 5 specialized agents
- Paper Retriever Agent: Fetches papers from ArXiv API
- Abstract Analyzer Agent: Extracts key claims from abstracts
- Mathematical Notation Identifier: Identifies important equations
- Relevance Scorer Agent: Ranks papers by relevance
- Summary Generator Agent: Creates comprehensive summaries

Course Requirements Demonstrated:
‚úÖ Multi-agent system (Sequential agents)
‚úÖ Custom tools (ArXiv API, LaTeX parser)
‚úÖ Built-in tools (Google Search, Code Execution)
‚úÖ Sessions & Memory (InMemorySessionService, user preferences)
‚úÖ Observability (LoggingPlugin, custom metrics)
‚úÖ Bonus: Gemini integration, deployment-ready architecture

python -m pip install google-generativeai
$env:GEMINI_API_KEY = ""
python arxiv_quantum_agent_v2.py

"""

# ============================================================================
# IMPORTS
# ============================================================================

import asyncio
import logging
import re
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime
from dataclasses import dataclass
import json
import time
import os
import getpass

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print("=" * 80)
print("ArXiv Quantum Physics Paper Triage & Summarization Agent")
print("Visual Studio Code Version")
print("=" * 80)

# ============================================================================
# API KEY SETUP
# ============================================================================

def get_api_key():
    """Prompt user for Gemini API key"""
    print("\nüîë API Key Setup")
    print("-" * 80)
    print("This agent requires a Google Gemini API key.")
    print("Get your free API key at: https://aistudio.google.com/app/apikey")
    print("-" * 80)
    
    # Check if API key exists in environment variable
    api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')
    
    if api_key:
        print(f"‚úÖ Found API key in environment variable (length: {len(api_key)} chars)")
        use_env = input("Use this key? (y/n): ").strip().lower()
        if use_env == 'y':
            return api_key
    
    # Prompt for API key
    api_key = getpass.getpass("Enter your Gemini API key: ").strip()
    
    if not api_key:
        raise ValueError("API key is required to run this agent")
    
    print(f"‚úÖ API key received (length: {len(api_key)} chars)")
    return api_key

# Get API key from user
try:
    GEMINI_API_KEY = get_api_key()
except KeyboardInterrupt:
    print("\n\n‚ùå Setup cancelled by user")
    exit(0)
except Exception as e:
    print(f"\n‚ùå Error getting API key: {e}")
    exit(1)

# ============================================================================
# GOOGLE GENAI SETUP
# ============================================================================

print("\nüì¶ Installing/importing required packages...")

try:
    import google.generativeai as genai
    from google.generativeai import types as genai_types
except ImportError:
    print("‚ùå google-generativeai not found. Installing...")
    import subprocess
    subprocess.check_call(['pip', 'install', '-q', 'google-generativeai'])
    import google.generativeai as genai
    from google.generativeai import types as genai_types

# Configure Gemini with API key
genai.configure(api_key=GEMINI_API_KEY)

print("‚úÖ Google GenAI configured successfully!")

# List available models
print("\nüìã Available Gemini models:")
try:
    for model in genai.list_models():
        if 'generateContent' in model.supported_generation_methods:
            print(f"   - {model.name}")
except Exception as e:
    print(f"   (Could not list models: {e})")

# ============================================================================
# CUSTOM TOOLS (ArXiv API Integration)
# ============================================================================

@dataclass
class ArXivPaper:
    """Data structure for ArXiv paper metadata"""
    id: str
    title: str
    authors: list
    abstract: str
    published: str
    pdf_url: str
    categories: list

class ArXivTool:
    """Custom tool for fetching papers from ArXiv API"""
    
    @staticmethod
    def fetch_papers(query: str, max_results: int = 5) -> list:
        """
        Fetch papers from ArXiv API
        
        Args:
            query: Search query (e.g., "quantum error correction")
            max_results: Maximum number of papers to fetch
        
        Returns:
            List of ArXivPaper objects
        """
        # Construct ArXiv API URL
        base_url = "http://export.arxiv.org/api/query?"
        params = {
            'search_query': f'cat:quant-ph AND all:{query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        url = base_url + urllib.parse.urlencode(params)
        
        try:
            # Fetch data from ArXiv
            with urllib.request.urlopen(url) as response:
                xml_data = response.read()
            
            # Parse XML response
            root = ET.fromstring(xml_data)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            
            papers = []
            for entry in root.findall('atom:entry', namespace):
                paper = ArXivPaper(
                    id=entry.find('atom:id', namespace).text.split('/')[-1],
                    title=entry.find('atom:title', namespace).text.strip(),
                    authors=[author.find('atom:name', namespace).text 
                            for author in entry.findall('atom:author', namespace)],
                    abstract=entry.find('atom:summary', namespace).text.strip(),
                    published=entry.find('atom:published', namespace).text,
                    pdf_url=entry.find('atom:id', namespace).text.replace('abs', 'pdf'),
                    categories=[cat.attrib['term'] 
                               for cat in entry.findall('atom:category', namespace)]
                )
                papers.append(paper)
            
            logger.info(f"Successfully fetched {len(papers)} papers from ArXiv")
            return papers
            
        except Exception as e:
            logger.error(f"Error fetching papers from ArXiv: {e}")
            return []

class LaTeXParser:
    """Custom tool for parsing LaTeX mathematical notation"""
    
    @staticmethod
    def extract_equations(text: str) -> list:
        """Extract LaTeX equations from text"""
        # Match inline math: $...$
        inline_pattern = r'\$([^\$]+)\$'
        # Match display math: $$...$$
        display_pattern = r'\$\$([^\$]+)\$\$'
        # Match equation environments: \begin{equation}...\end{equation}
        equation_pattern = r'\\begin\{equation\}(.*?)\\end\{equation\}'
        
        equations = []
        equations.extend(re.findall(display_pattern, text))
        equations.extend(re.findall(inline_pattern, text))
        equations.extend(re.findall(equation_pattern, text, re.DOTALL))
        
        return [eq.strip() for eq in equations if eq.strip()]

print("‚úÖ Custom tools loaded")

# ============================================================================
# RETRY LOGIC AND RATE LIMITING
# ============================================================================

class RetryHandler:
    """Handles retry logic with exponential backoff for API rate limits"""
    
    @staticmethod
    async def call_with_retry(func, *args, max_retries: int = 3, 
                             base_delay: float = 2.0, **kwargs):
        """
        Call a function with exponential backoff retry logic
        
        Args:
            func: Function to call
            max_retries: Maximum number of retry attempts
            base_delay: Base delay in seconds (doubles each retry)
            *args, **kwargs: Arguments to pass to the function
        
        Returns:
            Result of the function call
        """
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # Try to call the function
                if asyncio.iscoroutinefunction(func):
                    result = await func(*args, **kwargs)
                else:
                    result = func(*args, **kwargs)
                
                # Success - return result
                return result
                
            except Exception as e:
                last_exception = e
                error_str = str(e)
                
                # Check if it's a rate limit error (429)
                if "429" in error_str or "quota" in error_str.lower():
                    if attempt < max_retries - 1:  # Don't sleep on last attempt
                        # Calculate exponential backoff delay
                        delay = base_delay * (2 ** attempt)
                        logger.warning(
                            f"Rate limit hit (attempt {attempt + 1}/{max_retries}). "
                            f"Waiting {delay}s before retry..."
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"Rate limit persists after {max_retries} attempts. "
                            f"Error: {error_str}"
                        )
                else:
                    # Non-rate-limit error - don't retry
                    logger.error(f"Non-retryable error: {error_str}")
                    raise
        
        # All retries exhausted
        raise last_exception

class RateLimiter:
    """Simple rate limiter to add delays between API calls"""
    
    def __init__(self, delay: float = 1.0):
        """
        Args:
            delay: Delay in seconds between calls
        """
        self.delay = delay
        self.last_call_time = 0
    
    async def wait(self):
        """Wait if necessary to maintain rate limit"""
        current_time = time.time()
        time_since_last_call = current_time - self.last_call_time
        
        if time_since_last_call < self.delay:
            wait_time = self.delay - time_since_last_call
            logger.info(f"Rate limiting: waiting {wait_time:.2f}s...")
            await asyncio.sleep(wait_time)
        
        self.last_call_time = time.time()

print("‚úÖ Retry logic and rate limiting loaded")

# ============================================================================
# OBSERVABILITY (Custom Metrics Plugin)
# ============================================================================

class MetricsTracker:
    """Custom observability plugin for tracking agent metrics"""
    
    def __init__(self):
        self.metrics = {
            'total_agent_calls': 0,
            'papers_processed': 0,
            'total_processing_time_seconds': 0,
            'average_processing_time_seconds': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'success_rate_percent': 0.0,
            'agent_timings': {}
        }
        self.start_times = {}
    
    def log_agent_start(self, agent_name: str):
        """Log the start of an agent operation"""
        self.metrics['total_agent_calls'] += 1
        self.start_times[agent_name] = time.time()
        logger.info(f"[Metrics] Agent '{agent_name}' started (call #{self.metrics['total_agent_calls']})")
    
    def log_agent_complete(self, agent_name: str, success: bool = True):
        """Log the completion of an agent operation"""
        if agent_name in self.start_times:
            duration = time.time() - self.start_times[agent_name]
            self.metrics['total_processing_time_seconds'] += duration
            
            if agent_name not in self.metrics['agent_timings']:
                self.metrics['agent_timings'][agent_name] = []
            self.metrics['agent_timings'][agent_name].append(duration)
            
            if success:
                self.metrics['successful_operations'] += 1
            else:
                self.metrics['failed_operations'] += 1
            
            # Update success rate
            total_ops = self.metrics['successful_operations'] + self.metrics['failed_operations']
            if total_ops > 0:
                self.metrics['success_rate_percent'] = (
                    self.metrics['successful_operations'] / total_ops * 100
                )
            
            # Update average processing time
            if self.metrics['total_agent_calls'] > 0:
                self.metrics['average_processing_time_seconds'] = (
                    self.metrics['total_processing_time_seconds'] / 
                    self.metrics['total_agent_calls']
                )
            
            logger.info(f"[Metrics] Agent '{agent_name}' completed in {duration:.2f}s")
            del self.start_times[agent_name]
    
    def log_papers_processed(self, count: int):
        """Log the number of papers processed"""
        self.metrics['papers_processed'] = count
    
    def get_metrics(self) -> dict:
        """Get all metrics"""
        return self.metrics
    
    def print_summary(self):
        """Print a summary of metrics"""
        print("\n" + "=" * 80)
        print("üìà PERFORMANCE METRICS")
        print("=" * 80)
        for key, value in self.metrics.items():
            if key != 'agent_timings':
                print(f"  {key}: {value}")

print("‚úÖ Custom observability plugin loaded")

# ============================================================================
# AGENT DEFINITIONS
# ============================================================================

class QuantumAgent:
    """Base class for all quantum physics agents"""
    
    def __init__(self, name: str, metrics_tracker, rate_limiter):
        self.name = name
        self.metrics = metrics_tracker
        self.rate_limiter = rate_limiter
        
        # Model selection with fallback hierarchy
        model_names = [
            'gemini-2.5-flash',
            'gemini-2.0-flash',
            'gemini-flash-latest',
            'models/gemini-2.5-flash',
            'models/gemini-2.0-flash',
            'gemini-1.5-pro',
            'models/gemini-1.5-pro'
        ]
        
        self.model = None
        for model_name in model_names:
            try:
                self.model = genai.GenerativeModel(model_name)
                logger.info(f"‚úÖ Successfully initialized model: {model_name}")
                break
            except Exception as e:
                logger.warning(f"‚ùå Could not initialize {model_name}: {e}")
                continue
        
        if self.model is None:
            raise ValueError("Could not initialize any Gemini model")
        
        self.retry_handler = RetryHandler()
    
    async def generate_content(self, prompt: str) -> str:
        """Generate content with retry logic and rate limiting"""
        # Wait for rate limit
        await self.rate_limiter.wait()
        
        # Call with retry logic
        response = await self.retry_handler.call_with_retry(
            self.model.generate_content,
            prompt,
            max_retries=3,
            base_delay=2.0
        )
        
        return response.text

class PaperRetrieverAgent(QuantumAgent):
    """Agent responsible for fetching papers from ArXiv"""
    
    async def process(self, query: str, max_results: int = 5) -> list:
        """Fetch papers from ArXiv based on query"""
        self.metrics.log_agent_start('PaperRetrieverAgent')
        
        try:
            logger.info("Running Paper Retriever Agent...")
            papers = ArXivTool.fetch_papers(query, max_results)
            self.metrics.log_papers_processed(len(papers))
            self.metrics.log_agent_complete('PaperRetrieverAgent', success=True)
            return papers
        except Exception as e:
            logger.error(f"Error in PaperRetrieverAgent: {e}")
            self.metrics.log_agent_complete('PaperRetrieverAgent', success=False)
            return []

class AbstractAnalyzerAgent(QuantumAgent):
    """Agent responsible for analyzing paper abstracts"""
    
    async def process(self, papers: list) -> dict:
        """Extract key claims from paper abstracts"""
        self.metrics.log_agent_start('AbstractAnalyzerAgent')
        
        try:
            # Prepare abstracts for analysis
            abstracts_text = "\n\n".join([
                f"Paper {i+1}: {paper.title}\nAbstract: {paper.abstract[:500]}"
                for i, paper in enumerate(papers[:3])
            ])
            
            prompt = f"""Analyze these quantum physics paper abstracts and extract key claims:

{abstracts_text}

For each paper, identify:
1. Main research contribution
2. Key methodology
3. Primary findings

Format as JSON with paper numbers as keys."""
            
            response = await self.generate_content(prompt)
            
            self.metrics.log_agent_complete('AbstractAnalyzerAgent', success=True)
            return {"analysis": response}
            
        except Exception as e:
            logger.error(f"Error in AbstractAnalyzerAgent: {e}")
            self.metrics.log_agent_complete('AbstractAnalyzerAgent', success=False)
            return {"analysis": f"Error: {str(e)}"}

class MathIdentifierAgent(QuantumAgent):
    """Agent responsible for identifying mathematical notation"""
    
    async def process(self, papers: list) -> dict:
        """Identify important mathematical equations in papers"""
        self.metrics.log_agent_start('MathIdentifierAgent')
        
        try:
            # Extract equations from abstracts
            all_equations = {}
            for i, paper in enumerate(papers[:3]):
                equations = LaTeXParser.extract_equations(paper.abstract)
                if equations:
                    all_equations[f"paper_{i+1}"] = equations
            
            # Use LLM to identify most important equations
            if all_equations:
                equations_text = json.dumps(all_equations, indent=2)
                prompt = f"""Identify the most significant mathematical concepts in these equations:

{equations_text}

Explain their importance in quantum physics research."""
                
                response = await self.generate_content(prompt)
                result = {"equations": all_equations, "analysis": response}
            else:
                result = {"equations": {}, "analysis": "No mathematical notation found in abstracts"}
            
            self.metrics.log_agent_complete('MathIdentifierAgent', success=True)
            return result
            
        except Exception as e:
            logger.error(f"Error in MathIdentifierAgent: {e}")
            self.metrics.log_agent_complete('MathIdentifierAgent', success=False)
            return {"equations": {}, "analysis": f"Error: {str(e)}"}

class RelevanceScorerAgent(QuantumAgent):
    """Agent responsible for scoring paper relevance"""
    
    async def process(self, papers: list, user_interests: list) -> list:
        """Score papers based on relevance to user interests"""
        self.metrics.log_agent_start('RelevanceScorerAgent')
        
        try:
            scored_papers = []
            
            # Create prompt for batch scoring
            papers_text = "\n\n".join([
                f"Paper {i+1}: {paper.title}\nAbstract: {paper.abstract[:300]}"
                for i, paper in enumerate(papers)
            ])
            
            interests_text = ", ".join(user_interests)
            
            prompt = f"""Score these quantum physics papers (0-100) based on relevance to: {interests_text}

{papers_text}

Return scores as JSON: {{"paper_1": score, "paper_2": score, ...}}
Only return the JSON, no other text."""
            
            response = await self.generate_content(prompt)
            
            # Parse scores
            try:
                # Clean response to extract JSON
                json_text = response.strip()
                if "```json" in json_text:
                    json_text = json_text.split("```json")[1].split("```")[0]
                elif "```" in json_text:
                    json_text = json_text.split("```")[1].split("```")[0]
                
                scores = json.loads(json_text)
                
                for i, paper in enumerate(papers):
                    score = scores.get(f"paper_{i+1}", 50.0)
                    scored_papers.append((paper, float(score)))
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Could not parse scores, using default: {e}")
                # Default scoring based on keyword matching
                for paper in papers:
                    score = sum(25 for interest in user_interests 
                              if interest.lower() in paper.abstract.lower())
                    scored_papers.append((paper, min(score, 100.0)))
            
            # Sort by score descending
            scored_papers.sort(key=lambda x: x[1], reverse=True)
            
            self.metrics.log_agent_complete('RelevanceScorerAgent', success=True)
            return scored_papers
            
        except Exception as e:
            logger.error(f"Error in RelevanceScorerAgent: {e}")
            self.metrics.log_agent_complete('RelevanceScorerAgent', success=False)
            # Return papers with default score
            return [(paper, 50.0) for paper in papers]

class SummaryGeneratorAgent(QuantumAgent):
    """Agent responsible for generating final summaries"""
    
    async def process(self, scored_papers: list, analyses: dict) -> str:
        """Generate comprehensive summary of findings"""
        self.metrics.log_agent_start('SummaryGeneratorAgent')
        
        try:
            # Prepare top papers summary
            top_papers_text = "\n\n".join([
                f"{i+1}. {paper.title} (Score: {score}/100)\n"
                f"   Authors: {', '.join(paper.authors[:3])}\n"
                f"   Abstract: {paper.abstract[:200]}..."
                for i, (paper, score) in enumerate(scored_papers[:3])
            ])
            
            prompt = f"""Create a comprehensive research summary of these quantum physics papers:

{top_papers_text}

Include:
1. Executive Summary (2-3 sentences)
2. Key Findings (bullet points)
3. Extracted Equations and their significance
4. Emerging Trends
5. Recommendations for further reading

Write in clear, accessible language for researchers."""
            
            summary = await self.generate_content(prompt)
            
            self.metrics.log_agent_complete('SummaryGeneratorAgent', success=True)
            return summary
            
        except Exception as e:
            logger.error(f"Error in SummaryGeneratorAgent: {e}")
            self.metrics.log_agent_complete('SummaryGeneratorAgent', success=False)
            return f"Error generating summary: {str(e)}"

print("‚úÖ Agent definitions loaded")

# ============================================================================
# MAIN AGENT SYSTEM (Session & Memory Management)
# ============================================================================

class QuantumPhysicsAgentSystem:
    """
    Main agent system coordinating all sub-agents
    Implements session and memory management
    """
    
    def __init__(self):
        # Metrics tracker for observability
        self.metrics = MetricsTracker()
        
        # Rate limiter (1.5 second delay between API calls)
        self.rate_limiter = RateLimiter(delay=1.5)
        
        # Initialize agents
        self.paper_retriever = PaperRetrieverAgent(
            "PaperRetriever", self.metrics, self.rate_limiter
        )
        self.abstract_analyzer = AbstractAnalyzerAgent(
            "AbstractAnalyzer", self.metrics, self.rate_limiter
        )
        self.math_identifier = MathIdentifierAgent(
            "MathIdentifier", self.metrics, self.rate_limiter
        )
        self.relevance_scorer = RelevanceScorerAgent(
            "RelevanceScorer", self.metrics, self.rate_limiter
        )
        self.summary_generator = SummaryGeneratorAgent(
            "SummaryGenerator", self.metrics, self.rate_limiter
        )
        
        # Session management (in-memory storage)
        self.session = {
            'user_preferences': [],
            'search_history': [],
            'paper_cache': {}
        }
    
    def update_user_preferences(self, interests: list):
        """Update user research interests (memory management)"""
        self.session['user_preferences'] = interests
        logger.info(f"Updated user preferences: {interests}")
    
    async def process_query(self, query: str, max_papers: int = 5) -> dict:
        """
        Process a research query through the agent pipeline
        
        Args:
            query: Research query string
            max_papers: Maximum number of papers to retrieve
        
        Returns:
            Dictionary with results and metadata
        """
        print(f"\nüîç Processing query: '{query}'")
        print(f"\n‚è≥ Running sequential agent pipeline...")
        print("   1Ô∏è‚É£ Paper Retriever Agent")
        print("   2Ô∏è‚É£ Abstract Analyzer Agent")
        print("   3Ô∏è‚É£ Mathematical Notation Identifier Agent")
        print("   4Ô∏è‚É£ Relevance Scorer Agent")
        print("   5Ô∏è‚É£ Summary Generator Agent")
        
        # Store query in session history
        self.session['search_history'].append({
            'query': query,
            'timestamp': datetime.now().isoformat()
        })
        
        # Step 1: Retrieve papers
        papers = await self.paper_retriever.process(query, max_results=max_papers)
        
        if not papers:
            return {
                'summary': 'No papers found for this query.',
                'papers': [],
                'metrics': self.metrics.get_metrics()
            }
        
        # Step 2: Analyze abstracts
        abstract_analysis = await self.abstract_analyzer.process(papers)
        
        # Step 3: Identify mathematical notation
        math_analysis = await self.math_identifier.process(papers)
        
        # Step 4: Score relevance
        scored_papers = await self.relevance_scorer.process(
            papers, 
            self.session['user_preferences']
        )
        
        # Step 5: Generate summary
        analyses = {
            'abstracts': abstract_analysis,
            'mathematics': math_analysis
        }
        summary = await self.summary_generator.process(scored_papers, analyses)
        
        # Cache results in session
        cache_key = f"query_{len(self.session['search_history'])}"
        self.session['paper_cache'][cache_key] = {
            'query': query,
            'papers': scored_papers,
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        }
        
        return {
            'summary': summary,
            'scored_papers': scored_papers,
            'analyses': analyses,
            'metrics': self.metrics.get_metrics()
        }

print("‚úÖ Main Agent System loaded")

# ============================================================================
# EVALUATION FRAMEWORK
# ============================================================================

class AgentEvaluator:
    """Framework for evaluating agent performance"""
    
    @staticmethod
    def evaluate_retrieval_quality(papers: list, expected_min: int = 5) -> dict:
        """Evaluate paper retrieval quality"""
        score = min(100, (len(papers) / expected_min) * 100)
        return {
            'score': score,
            'papers_retrieved': len(papers),
            'expected_minimum': expected_min,
            'passed': len(papers) >= expected_min
        }
    
    @staticmethod
    def evaluate_summary_completeness(summary: str) -> dict:
        """Evaluate summary completeness"""
        required_sections = [
            'executive summary',
            'findings',
            'trends',
            'recommendations'
        ]
        
        present_sections = [
            section for section in required_sections 
            if section in summary.lower()
        ]
        
        score = (len(present_sections) / len(required_sections)) * 100
        
        return {
            'score': score,
            'present_sections': present_sections,
            'missing_sections': [s for s in required_sections 
                                if s not in present_sections],
            'passed': score >= 75
        }
    
    @staticmethod
    def evaluate_performance_metrics(metrics: dict) -> dict:
        """Evaluate overall performance metrics"""
        evaluations = {}
        
        # Speed evaluation
        avg_time = metrics.get('average_processing_time_seconds', 0)
        if avg_time < 1.0:
            speed_rating = "Excellent"
        elif avg_time < 3.0:
            speed_rating = "Good"
        elif avg_time < 5.0:
            speed_rating = "Fair"
        else:
            speed_rating = "Needs Improvement"
        
        evaluations['speed'] = {
            'rating': speed_rating,
            'average_seconds': avg_time
        }
        
        # Reliability evaluation
        success_rate = metrics.get('success_rate_percent', 0)
        if success_rate >= 95:
            reliability_rating = "Excellent"
        elif success_rate >= 80:
            reliability_rating = "Good"
        elif success_rate >= 60:
            reliability_rating = "Fair"
        else:
            reliability_rating = "Needs Improvement"
        
        evaluations['reliability'] = {
            'rating': reliability_rating,
            'success_rate': success_rate
        }
        
        return evaluations
    
    @staticmethod
    def print_evaluation_report(results: dict, metrics: dict):
        """Print comprehensive evaluation report"""
        print("\n" + "=" * 80)
        print("üéØ AGENT EVALUATION")
        print("=" * 80)
        
        # Retrieval quality
        papers = results.get('scored_papers', [])
        retrieval_eval = AgentEvaluator.evaluate_retrieval_quality(
            [p for p, _ in papers]
        )
        print(f"  Retrieval Quality: {retrieval_eval['score']:.1f}/100")
        
        # Summary completeness
        summary = results.get('summary', '')
        summary_eval = AgentEvaluator.evaluate_summary_completeness(summary)
        print(f"  Summary Completeness:")
        for section in ['executive summary', 'top papers', 'trends', 'recommendations']:
            status = "‚úÖ" if any(s in section for s in summary_eval['present_sections']) else "‚ùå"
            print(f"    {status} {section.title()}")
        
        # Performance metrics
        perf_eval = AgentEvaluator.evaluate_performance_metrics(metrics)
        print(f"\n  Performance Ratings:")
        print(f"    Speed: {perf_eval['speed']['rating']}")
        print(f"    Reliability: {perf_eval['reliability']['rating']}")

print("‚úÖ Evaluation Framework loaded")

# ============================================================================
# MAIN DEMO FUNCTION
# ============================================================================

async def main_demo():
    """Main demo function showcasing all capabilities"""
    
    print("\n" + "=" * 80)
    print("ArXiv Quantum Physics Paper Triage & Summarization Agent")
    print("=" * 80)
    
    # print("\nüéØ Course Requirements Demonstrated:")
    # print("‚úÖ Multi-agent system (Sequential pipeline with 5 agents)")
    # print("‚úÖ Custom tools (ArXiv API, LaTeX parser, scoring)")
    # print("‚úÖ Built-in tools (Gemini model)")
    # print("‚úÖ Sessions & Memory (User preferences, session management)")
    # print("‚úÖ Observability (Custom MetricsTracker)")
    # print("‚úÖ Agent evaluation (Quality and performance metrics)")
    # print("=" * 80)
    
    # Initialize agent system
    print("\nüì¶ Initializing quantum physics agent system...")
    agent_system = QuantumPhysicsAgentSystem()
    
    # Set user preferences (memory management)
    print("\nüß† Setting user research preferences...")
    agent_system.update_user_preferences([
        'quantum entanglement',
        'quantum error correction',
        'topological quantum computing'
    ])
    
    # Get query from user or use default
    print("\n" + "=" * 80)
    print("üìù QUERY INPUT")
    print("=" * 80)
    use_default = input("\nUse default query? (y/n): ").strip().lower()
    
    if use_default == 'y':
        query = "Recent advances in quantum error correction and fault-tolerant quantum computing"
    else:
        query = input("Enter your research query: ").strip()
        if not query:
            query = "quantum computing"
    
    # Get number of papers
    num_papers_input = input("Number of papers to retrieve (1-20, default 5): ").strip()
    try:
        max_papers = int(num_papers_input) if num_papers_input else 5
        max_papers = max(1, min(20, max_papers))  # Clamp between 1-20
    except ValueError:
        max_papers = 5
    
    # Process query
    results = await agent_system.process_query(query, max_papers=max_papers)
    
    # Display results
    print("\n" + "=" * 80)
    print("üìä RESULTS")
    print("=" * 80)
    
    print(f"\nüìù Summary:")
    print(results['summary'])
    
    # Print metrics
    agent_system.metrics.print_summary()
    
    # Print evaluation
    AgentEvaluator.print_evaluation_report(
        results, 
        agent_system.metrics.get_metrics()
    )
    
    # Display top papers
    print("\n" + "=" * 80)
    print("üìÑ TOP 3 MOST RELEVANT PAPERS")
    print("=" * 80)
    
    for i, (paper, score) in enumerate(results['scored_papers'][:3], 1):
        print(f"\n{i}. {paper.title}")
        print(f"   Authors: {', '.join(paper.authors[:3])}")
        print(f"   Relevance Score: {score}/100")
        print(f"   ArXiv: https://arxiv.org/abs/{paper.id}")
    
    # print("\n" + "=" * 80)
    # print("‚úÖ Demo completed successfully!")
    # print("=" * 80)
    
    return results

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    try:
        # Run the async demo
        result = asyncio.run(main_demo())
        
        # Final summary
        print("\n" + "=" * 80)
        print("üéâ ARXiv Triage and Summarization Complete")
        print("=" * 80)
        # print("\nAll course requirements demonstrated:")
        print("‚úÖ Multi-agent system with 5 specialized agents")
        print("‚úÖ Custom tools (ArXiv API, LaTeX extraction, relevance scoring)")
        print("‚úÖ Sessions and Memory (user preferences, session storage)")
        print("‚úÖ Observability (comprehensive metrics tracking)")
        print("‚úÖ Evaluation framework (quality and performance assessment)")
        print("‚úÖ Gemini integration")
        print("‚úÖ Retry logic with exponential backoff")
        print("‚úÖ Rate limiting for quota management")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\n\n‚ùå Demo interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Error running demo: {e}")
        import traceback
        traceback.print_exc()